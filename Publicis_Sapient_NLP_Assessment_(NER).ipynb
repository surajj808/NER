{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ky5Ehktlb_4-"
      },
      "outputs": [],
      "source": [
        "# Importing all the needed libraries for the project\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the Dataset\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/NER Project/ner_dataset.csv', encoding='latin1')\n",
        "df.pop('POS')\n",
        "df = df.fillna(method=\"ffill\")\n",
        "# df.head()\n",
        "# df.shape"
      ],
      "metadata": {
        "id": "lkKmjaC2cGQG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.Word, df.Tag, test_size=0.2, random_state=42)\n",
        "# x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "fOXweX7p7iP-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Preparing Training Data\n",
        "\n",
        "# Creating a list of words & Tags for the further steps\n",
        "\n",
        "words = x_train.tolist()\n",
        "ner_tags = y_train.tolist()\n",
        "\n",
        "# Step 1: Encoding\n",
        "word_to_index = {word: i + 1 for i, word in enumerate(set(words))}\n",
        "ner_to_index = {ner: i + 1 for i, ner in enumerate(set(ner_tags))}\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "ner_labels_encoded = label_encoder.fit_transform(ner_tags)\n",
        "\n",
        "word_indices = [word_to_index[word] for word in words]\n",
        "ner_indices = [ner_to_index[ner] for ner in ner_tags]\n",
        "\n",
        "# Step 2: Padding\n",
        "max_length = max(len(word_indices), len(ner_indices))\n",
        "word_indices_padded = np.pad(word_indices, (0, max_length - len(word_indices)), mode='constant')\n",
        "ner_indices_padded = np.pad(ner_indices, (0, max_length - len(ner_indices)), mode='constant')\n",
        "\n",
        "# Step 3: Splitting the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(word_indices_padded, ner_indices_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Preparing the inputs for the Model.\n",
        "# A CNN Model needs inputs like word_indices_padded_reshaped, ner_indices_padded_reshaped, vocab_size, num_classes, max_length\n",
        "word_indices_padded_reshaped = np.expand_dims(word_indices_padded, axis=-1)\n",
        "ner_indices_padded_reshaped = np.expand_dims(ner_indices_padded, axis=-1)\n",
        "vocab_size = len(word_to_index) + 1\n",
        "num_classes = len(set(df[\"Tag\"]))"
      ],
      "metadata": {
        "id": "ykWnnaSTci-e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Building a CNN Model for prediction:\n",
        "\n",
        "# Define model parameters\n",
        "num_classes = len(set(ner_labels_encoded))\n",
        "embedding_dim = 100\n",
        "num_filters = 128\n",
        "kernel_size = 3\n",
        "\n",
        "def build_model(input_dim, output_dim, max_seq_length, embedding_dim=embedding_dim, num_filters=num_filters, kernel_size=kernel_size):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding_layer = Embedding(input_dim=input_dim, output_dim=embedding_dim, input_length=max_length)(inputs)\n",
        "    conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu', padding='same')(embedding_layer)\n",
        "    pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
        "    outputs = Dense(output_dim, activation='softmax')(pooling_layer)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_model(input_dim=vocab_size, output_dim=num_classes, max_seq_length=max_length,\n",
        "                    embedding_dim=embedding_dim, num_filters=num_filters, kernel_size=kernel_size)\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(word_indices_padded_reshaped, ner_labels_encoded, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTxPtuTueHbU",
        "outputId": "ce8bef3d-e9e1-4de1-c1b3-15b10e30afc3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "10486/10486 [==============================] - 605s 58ms/step - loss: 0.2385 - accuracy: 0.9369 - val_loss: 0.1880 - val_accuracy: 0.9452\n",
            "Epoch 2/5\n",
            "10486/10486 [==============================] - 593s 57ms/step - loss: 0.1544 - accuracy: 0.9543 - val_loss: 0.1865 - val_accuracy: 0.9464\n",
            "Epoch 3/5\n",
            "10486/10486 [==============================] - 536s 51ms/step - loss: 0.1436 - accuracy: 0.9562 - val_loss: 0.1883 - val_accuracy: 0.9464\n",
            "Epoch 4/5\n",
            "10486/10486 [==============================] - 534s 51ms/step - loss: 0.1395 - accuracy: 0.9566 - val_loss: 0.1910 - val_accuracy: 0.9461\n",
            "Epoch 5/5\n",
            "10486/10486 [==============================] - 546s 52ms/step - loss: 0.1371 - accuracy: 0.9568 - val_loss: 0.1963 - val_accuracy: 0.9455\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b191ce5b700>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Preparing Testing Data\n",
        "\n",
        "# Creating a list of words & Tags for the further steps\n",
        "\n",
        "words_test = x_test.tolist()\n",
        "ner_tags_test = y_test.tolist()\n",
        "\n",
        "# Step 1: Encoding\n",
        "word_to_index_test = {word: i + 1 for i, word in enumerate(set(words_test))}\n",
        "ner_to_index_test = {ner: i + 1 for i, ner in enumerate(set(ner_tags_test))}\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "ner_labels_encoded_test = label_encoder.fit_transform(ner_tags_test)\n",
        "\n",
        "word_indices_test = [word_to_index_test[word] for word in words_test]\n",
        "ner_indices_test = [ner_to_index_test[ner] for ner in ner_tags_test]\n",
        "\n",
        "# Step 2: Padding\n",
        "word_indices_padded_test = np.pad(word_indices_test, (0, max_length - len(word_indices_test)), mode='constant')\n",
        "ner_indices_padded_test = np.pad(ner_indices_test, (0, max_length - len(ner_indices_test)), mode='constant')\n",
        "\n",
        "word_indices_padded_reshaped_test = np.expand_dims(word_indices_padded_test, axis=-1)"
      ],
      "metadata": {
        "id": "hfZNPeCfdX6O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model performance using metrics\n",
        "\n",
        "y_pred = model.predict(word_indices_padded_reshaped_test)\n",
        "\n",
        "# def evaluate_model(model, x_test, y_test):\n",
        "\n",
        "#     y_pred = model.predict(x_test)\n",
        "#     # Convert predicted probabilities to class labels\n",
        "#     y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "#     accuracy = np.mean(y_pred_labels == y_test)\n",
        "#     return accuracy\n",
        "\n",
        "# accuracy = evaluate_model(model, word_indices_padded_reshaped_test, ner_labels_encoded_test)\n",
        "# print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVy3rFphyVE3",
        "outputId": "c38bced0-c71a-4a7b-977d-fd65e160e4ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26215/26215 [==============================] - 49s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc = accuracy_score(ner_labels_encoded_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ],
      "metadata": {
        "id": "P4Grevqfp3sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict NER Tags for the words in a sentence given by the User.\n",
        "\n",
        "def predict_ner_tags(model, tokenizer, sentence):\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokens = tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(tokens, maxlen=max_length, padding='post')\n",
        "\n",
        "    predictions = model.predict(padded_tokens)\n",
        "\n",
        "    predicted_ner_tags = [np.argmax(pred) for pred in predictions[0]]\n",
        "    return predicted_ner_tags\n",
        "\n",
        "user_input = str(input('Enter a Sentence: '))\n",
        "predicted_tags = predict_ner_tags(model, tokenizer, user_input)\n",
        "print(\"Predicted NER tags:\", predicted_tags)"
      ],
      "metadata": {
        "id": "ndW7ZB366yfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKXEARnitz8R",
        "outputId": "0fa36f39-4e1b-47f1-c535-50d20f26d64a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn2B_b3gt7Ur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}